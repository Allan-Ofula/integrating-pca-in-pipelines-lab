























# Your code here

# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# Loading data
data = pd.read_csv('otto_group.csv')


# Your code here

# Inspecting dataset
print("Shape of dataset:", data.shape)
data.head()


# Your code here

# Checking for missing values
missing = data.isnull().sum().sum()
print("Total missing values:", missing)


# Your code here

# Checking target variable
print("Target distribution:\n", data['target'].value_counts())


# Your code here

# Plotting distribution of the target
plt.figure(figsize=(10, 5))
sns.countplot(x='target', data=data)
plt.title("Distribution of Target Classes")
plt.xticks(rotation=45)
plt.show()





# Your code here

# Plotting histograms for selected features (first 10)
data.iloc[:, 1:11].hist(figsize=(15, 10), bins=30)
plt.suptitle("Histograms of First 10 Features")
plt.show()





# Your code here

# Summary statistics
data.describe().T








# Your code here

# Dropping id and target for correlation matrix
features_only = data.drop(['id', 'target'], axis=1)

# Computing correlation matrix
corr = features_only.corr()

# Plotting heatmap for first 20 features 
plt.figure(figsize=(12, 10))
sns.heatmap(corr.iloc[:20, :20], cmap='coolwarm', annot=False)
plt.title("Correlation Heatmap (First 20 Features)")
plt.show()





# Your code here

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Standardizing the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(features_only)


# Your code here

# Applying PCA to determine number of components for 80% variance
pca = PCA(n_components=0.80)
X_pca = pca.fit_transform(X_scaled)

# Results
print(f"Original feature count: {features_only.shape[1]}")
print(f"Reduced feature count after PCA (80% variance): {X_pca.shape[1]}")


# Plotting cumulative explained variance
plt.figure(figsize=(10, 5))
plt.plot(np.cumsum(PCA().fit(X_scaled).explained_variance_ratio_), marker='o')
plt.axhline(y=0.80, color='r', linestyle='--')
plt.title('Cumulative Explained Variance by PCA Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True)
plt.show()





# Your code here

from sklearn.model_selection import train_test_split

# Target labels
X = X_pca  
y = data['target']


# Your code here

# Encoding target labels if not numeric
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.40, random_state=42, stratify=y_encoded
)

print("Train set size:", X_train.shape)
print("Test set size:", X_test.shape)








# Your code here

#Importing libraries
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


# Your code here

# Pipeline with Scaling, PCA, Logistic Regression
baseline_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.80)),
    ('logreg', LogisticRegression(random_state=123, max_iter=1000))
])

# Fitting the pipeline
baseline_pipeline.fit(X_train, y_train)


# Your code here

# Predict and evaluate
y_pred = baseline_pipeline.predict(X_test)
baseline_accuracy = accuracy_score(y_test, y_pred)
print("Baseline Logistic Regression Accuracy:", baseline_accuracy)








# Your code here
# ⏰ This cell may take several minutes to run

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Linear SVM Pipeline
svm_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.80)),
    ('svm', SVC(kernel='linear', random_state=123))
])

# Decision Tree Pipeline
tree_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.80)),
    ('tree', DecisionTreeClassifier(random_state=123))
])

# Random Forest Pipeline
rf_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.80)),
    ('rf', RandomForestClassifier(random_state=123))
])

# Fitting and scoring each
svm_pipeline.fit(X_train, y_train)
tree_pipeline.fit(X_train, y_train)
rf_pipeline.fit(X_train, y_train)

print("Linear SVM Accuracy:", accuracy_score(y_test, svm_pipeline.predict(X_test)))
print("Decision Tree Accuracy:", accuracy_score(y_test, tree_pipeline.predict(X_test)))
print("Random Forest Accuracy:", accuracy_score(y_test, rf_pipeline.predict(X_test)))











# Your code here 

# imports
from sklearn.model_selection import GridSearchCV


# Your code here
# ⏰ This cell may take a long time to run!
rf_param_grid = {
    'rf__n_estimators': [50, 100, 150],
    'rf__max_depth': [None, 10, 20],
    'rf__min_samples_split': [2, 5],
    'rf__min_samples_leaf': [1, 2]
}

# Pipeline with Grid Search
rf_grid_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.80)),
    ('rf', RandomForestClassifier(random_state=123))
])

rf_grid_search = GridSearchCV(rf_grid_pipeline, rf_param_grid, cv=3, n_jobs=-1, verbose=1)
rf_grid_search.fit(X_train, y_train)

print("Best RF Parameters:", rf_grid_search.best_params_)
print("Best RF Accuracy:", rf_grid_search.best_score_)





# Your code here 

# Results
rf_results = pd.DataFrame(rf_grid_search.cv_results_)
rf_results.sort_values(by='mean_test_score', ascending=False).head()





# Your code here
# ⏰ This cell may take several minutes to run
from sklearn.ensemble import AdaBoostClassifier

ada_param_grid = {
    'ada__n_estimators': [50, 100, 150],
    'ada__learning_rate': [0.01, 0.1, 1]
}

ada_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.80)),
    ('ada', AdaBoostClassifier(random_state=123))
])

ada_grid_search = GridSearchCV(ada_pipeline, ada_param_grid, cv=3, n_jobs=-1, verbose=1)
ada_grid_search.fit(X_train, y_train)

print("Best AdaBoost Parameters:", ada_grid_search.best_params_)
print("Best AdaBoost Accuracy:", ada_grid_search.best_score_)





# Your code here 

# Results
ada_results = pd.DataFrame(ada_grid_search.cv_results_)
ada_results.sort_values(by='mean_test_score', ascending=False).head()





# Your code here
# ⏰ This cell may take a very long time to run!
svm_param_grid = {
    'svm__C': [1, 10],
    'svm__gamma': ['scale', 0.1],
    'svm__kernel': ['rbf']
}

svm_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.80)),
    ('svm', SVC())
])

svm_grid_search = GridSearchCV(svm_pipeline, svm_param_grid, cv=3, n_jobs=-1, verbose=1)
svm_grid_search.fit(X_train, y_train)

print("Best SVM Parameters:", svm_grid_search.best_params_)
print("Best SVM Accuracy:", svm_grid_search.best_score_)





# Your code here 

# Results
svm_results = pd.DataFrame(svm_grid_search.cv_results_)
svm_results.sort_values(by='mean_test_score', ascending=False).head()






